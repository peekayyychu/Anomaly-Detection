Isolation Forest is particularly well-suited for real-time anomaly detection in streaming data due to the following reasons:

1. Efficiency in High-Dimensional Data: Isolation Forest isolates anomalies by recursively partitioning the data. Anomalies, which are few and different, require fewer partitions, making the algorithm highly efficient even with large and high-dimensional data streams.

2. Scalability: It has low computational complexity and can handle large datasets by building trees on random subsets (subsampling). This makes it ideal for real-time applications where new data arrives continuously.

3. Ability to Detect Both Local and Global Anomalies: Isolation Forest can detect both point anomalies (outliers) and collective anomalies (patterns that deviate from expected behavior), which is crucial for identifying unusual patterns in system metrics or financial data.

4. Concept Drift Adaptability: Since it works on random subsamples, you can periodically re-train the model to adapt to changes in data distribution (concept drift) without needing to retain the entire dataset.

Effectiveness:
1. No Assumptions on Data Distribution: Unlike other techniques that assume normality, Isolation Forest doesn't require prior knowledge of data distribution, which is ideal when streaming data contains complex, non-linear relationships.

2. Faster Detection of Anomalies: Since anomalies are isolated faster, the algorithm can detect them in fewer operations, which makes it highly effective for real-time detection where quick decisions are critical.

Overall, Isolation Forest balances efficiency, scalability, and the ability to handle dynamic, real-time data streams, making it a highly effective choice for this problem.